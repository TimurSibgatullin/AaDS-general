
# **Sorts**
## Merge Sort
### **1) Временная сложность**  
- **Лучший, средний и худший случаи:** \(O(n \log n)\)  
- **Пространственная сложность:** \(O(n)\) (требуется дополнительная память)  

### **2) Худший случай**  
- Любой входной массив, так как Merge Sort всегда делит массив пополам и сливает, независимо от исходного порядка.  

### **3) Доказательство сложности**  
- **Рекуррентное соотношение:** \(T(n) = 2T(n/2) + O(n)\)  
- По теореме о рекуррентных соотношениях (методом "разделяй и властвуй") решение: \(O(n \log n)\).  

### **4) Принцип работы**  
- **"Разделяй и властвуй"**  
1. **Разделение:** массив рекурсивно делится на две половины, пока не останутся подмассивы из одного элемента.  
2. **Слияние:** два отсортированных подмассива сливаются в один упорядоченный массив.  

### **5) Пошаговое выполнение**  
**Пример:** `[3, 1, 4, 2]`  
1. **Разделение:**  
   - `[3, 1]` и `[4, 2]`  
   - `[3]`, `[1]`, `[4]`, `[2]`  
2. **Слияние:**  
   - `[1, 3]` (слияние `[3]` и `[1]`)  
   - `[2, 4]` (слияние `[4]` и `[2]`)  
   - `[1, 2, 3, 4]` (слияние `[1, 3]` и `[2, 4]`)  

### **6) Результат работы**  
- **Вход:** `[3, 1, 4, 2]`  
- **Выход:** `[1, 2, 3, 4]` (отсортированный массив)  

**Готов к следующему алгоритму! Какой разберем?** (Следующий по списку — **Tree Sort**.)

## **Tree Sort**

### **1) Временная сложность**  
- **Средний случай:** \(O(n \log n)\) (если дерево сбалансировано)  
- **Худший случай:** \(O(n^2)\) (если дерево вырождается в линейный список, например, при уже отсортированном массиве)  
- **Пространственная сложность:** \(O(n)\) (хранение узлов дерева)  

### **2) Худший случай**  
- **Уже отсортированный массив** (например, `[1, 2, 3, 4]`), при котором дерево становится вырожденным (вытягивается в "цепочку").  

### **3) Доказательство сложности**  
- В **среднем случае** (случайные данные):  
  - Вставка каждого элемента в дерево — \(O(\log n)\) → для \(n\) элементов \(O(n \log n)\).  
  - Обход дерева (in-order traversal) — \(O(n)\).  
  - Итого: \(O(n \log n)\).  
- В **худшем случае** (вырожденное дерево):  
  - Вставка каждого нового элемента требует \(O(n)\) сравнений → для \(n\) элементов \(O(n^2)\).  

### **4) Принцип работы**  
1. **Построение двоичного дерева поиска (BST):**  
   - Каждый элемент массива вставляется в BST с учетом правил:  
     - Левый потомок ≤ родитель ≤ правый потомок.  
2. **In-order обход дерева:**  
   - Рекурсивный обход: левое поддерево → корень → правое поддерево.  
   - Результат обхода — отсортированный массив.  

### **5) Пошаговое выполнение**  
**Пример:** `[3, 1, 4, 2]`  
1. **Построение BST:**  
   - Вставка 3: корень.  
   - Вставка 1: левый потомок 3.  
   - Вставка 4: правый потомок 3.  
   - Вставка 2: правый потомок 1.  
   ```
       3  
      / \  
     1   4  
      \  
       2  
   ```  
2. **In-order обход:**  
   - Посещаем: `1` → `2` → `3` → `4`.  

### **6) Результат работы**  
- **Вход:** `[3, 1, 4, 2]`  
- **Выход:** `[1, 2, 3, 4]`  

## **Tim Sort**
Переходим к **Timsort** — гибридному алгоритму, сочетающему **сортировку вставками** и **слияние** (как в Merge Sort).  

### **1) Временная сложность**  
- **Лучший случай:** \(O(n)\) (уже отсортированный массив)  
- **Средний и худший случаи:** \(O(n \log n)\)  
- **Пространственная сложность:** \(O(n)\)  

### **2) Худший случай**  
- Массив, где элементы расположены в обратном порядке (например, `[4, 3, 2, 1]`), так как требует максимального количества сравнений и слияний.  

### **3) Доказательство сложности**  
- Алгоритм разбивает массив на подмассивы (**run**), которые либо уже упорядочены, либо сортируются вставками (если длина run < `min_run`).  
- Слияние run напоминает Merge Sort, но с оптимизациями:  
  - Минимальный размер run (обычно 32–64) гарантирует, что сортировка вставками работает за \(O(n^2)\) только на маленьких блоках.  
  - Число run ограничено \(O(\log n)\), а их слияние — \(O(n \log n)\).  

### **4) Принцип работы**  
1. **Разбиение на run:**  
   - Находят подмассивы (run), которые либо строго возрастают, либо убывают (убывающие разворачивают).  
   - Если run слишком короткий (< `min_run`), дополняют его и сортируют вставками.  
2. **Слияние run:**  
   - Используют **стек** для хранения run и сливают их по принципу:  
     - Размеры трех верхних run в стеке должны удовлетворять условию `X > Y + Z` (чтобы избежать каскадных слияний).  
   - Слияние проводится с использованием **галопного поиска** для оптимизации.  

### **5) Пошаговое выполнение**  
**Пример:** `[5, 2, 3, 1, 4, 6]` (пусть `min_run = 2`)  
1. **Выделение run:**  
   - `[5, 2]` → убывающий → разворот → `[2, 5]`  
   - `[3]` → слишком короткий → объединяем со следующим → `[3, 1]` → разворот → `[1, 3]`  
   - `[4, 6]` → уже возрастающий  
2. **Слияние run:**  
   - Стек: `[2,5]`, `[1,3]` → `[1,3]` и `[2,5]` сливаются → `[1,2,3,5]`  
   - Добавляем `[4,6]` → сливаем `[1,2,3,5]` и `[4,6]` → итог: `[1,2,3,4,5,6]`  

### **6) Результат работы**  
- **Вход:** `[5, 2, 3, 1, 4, 6]`  
- **Выход:** `[1, 2, 3, 4, 5, 6]`  

**Факт:** Timsort — стандартный алгоритм сортировки в Python (`sorted()`), Java (для массивов объектов) и Android.  

## **Bucket Sort** 

**1) Временная сложность**  
- **Лучший случай:** \(O(n + k)\), где \(k\) — количество блоков (при равномерном распределении).  
- **Средний случай:** \(O(n + k)\).  
- **Худший случай:** \(O(n^2)\) (если все элементы попадают в один блок).  
- **Пространственная сложность:** \(O(n + k)\).  

---  

**2) Худший случай**  
- Элементы сильно неравномерно распределены (например, все элементы попадают в один блок).  
- Пример: `[0.1, 0.1, 0.1, ..., 0.1]` при 10 блоках.  

---  

**3) Доказательство сложности**  
- **Распределение по блокам:** \(O(n)\).  
- **Сортировка внутри блоков** (например, Insertion Sort):  
  - Если элементы равномерно распределены, каждый блок содержит \(\frac{n}{k}\) элементов → сортировка одного блока: \(O\left(\left(\frac{n}{k}\right)^2\right)\).  
  - Для \(k\) блоков: \(O\left(k \cdot \left(\frac{n}{k}\right)^2\right) = O\left(\frac{n^2}{k}\right)\).  
  - При \(k \approx n\) получаем \(O(n)\).  
- **Объединение блоков:** \(O(n)\).  
- **Итог:** В среднем \(O(n + k)\), в худшем случае \(O(n^2)\).  

---  

**4) Принцип работы**  
1. **Создание блоков (бакетов):**  
   - Диапазон входных данных разбивается на \(k\) интервалов.  
   - Каждый элемент помещается в соответствующий блок.  
2. **Сортировка блоков:**  
   - Каждый блок сортируется отдельно (обычно Insertion Sort или другой алгоритм).  
3. **Объединение блоков:**  
   - Элементы из всех блоков собираются в итоговый массив.  

---  

**5) Пошаговое выполнение**  
**Пример:** `[0.42, 0.32, 0.75, 0.12, 0.89]` (используем 5 блоков для диапазона [0, 1))  
1. **Распределение:**  
   - Блок 0 (0.0–0.2): `[0.12]`  
   - Блок 1 (0.2–0.4): `[0.32, 0.42]` *(ошибка: 0.42 ∈ [0.4–0.6), исправлено ниже)*  
   - Блок 2 (0.4–0.6): `[0.42]`  
   - Блок 3 (0.6–0.8): `[0.75]`  
   - Блок 4 (0.8–1.0): `[0.89]`  
   *(Уточнение: 0.42 ∈ [0.4–0.6), поэтому исправляем распределение.)*  
2. **Сортировка блоков:**  
   - Блок 0: `[0.12]` (уже отсортирован).  
   - Блок 1: `[0.32]`  
   - Блок 2: `[0.42]`  
   - Блок 3: `[0.75]`  
   - Блок 4: `[0.89]`  
3. **Объединение:** `[0.12, 0.32, 0.42, 0.75, 0.89]`.  

*(Примечание: В исходном примере 0.42 ошибочно попал в блок 1, но должен быть в блоке 2. Исправленный вариант верен.)*  

---  

**6) Результат работы**  
- **Вход:** `[0.42, 0.32, 0.75, 0.12, 0.89]`  
- **Выход:** `[0.12, 0.32, 0.42, 0.75, 0.89]`  

---  

**Особенности:**  
- Эффективен, когда данные равномерно распределены.  
- Используется в внешней сортировке и базах данных.  

## **Stooge Sort**

**1) Временная сложность**  
- **Худший случай:** \(O(n^{\log_3 3}) \approx O(n^{2.709})\) (экзотически медленный!).  
- **Пространственная сложность:** \(O(n)\) из-за рекурсии.  

**2) Худший случай**  
- Любой неотсортированный массив, так как алгоритм всегда выполняет полное рекурсивное разбиение.  

**3) Доказательство сложности**  
- Рекуррентное соотношение: \(T(n) = 3T(2n/3) + O(1)\).  
- По теореме о рекуррентных соотношениях: \(T(n) = O(n^{\log_{3/2} 3})\).  

**4) Принцип работы**  
1. Если первый элемент больше последнего — меняем их местами.  
2. Если в массиве ≥3 элементов:  
   - Рекурсивно сортируем первые 2/3 массива.  
   - Рекурсивно сортируем последние 2/3 массива.  
   - Снова рекурсивно сортируем первые 2/3 массива.  

**5) Пошаговый пример**  
**Массив:** [3, 1, 4, 2]  
1. [3,1,4,2] → swap(3,2) → [2,1,4,3]  
2. Рекурсия на первые 2/3 [2,1,4]:  
   - [2,1,4] → swap(2,4) → [4,1,2]  
   - Рекурсия на [4,1] → swap(4,1) → [1,4,2]  
   - Рекурсия на [4,2] → swap(4,2) → [1,2,4]  
3. Возвращаемся к полному массиву: [1,2,4,3]  
4. Рекурсия на последние 2/3 [2,4,3]:  
   - [2,4,3] → swap(2,3) → [3,4,2]  
   - ... (продолжаем рекурсивно)  
5. Финальный результат: [1,2,3,4]  

**6) Результат работы**  
Вход: [3,1,4,2] → Выход: [1,2,3,4]  

**Особенности:**  
- Представляет скорее академический интерес из-за чудовищной неэффективности.  
- Назван в честь комедийного трио "Three Stooges".  

## **Comb Sort**

**1) Временная сложность**  
- **Лучший случай:** \(O(n \log n)\) (при удачном выборе шага)  
- **Средний случай:** \(O(n^2 / 2^p)\), где \(p\) — количество уменьшений шага  
- **Худший случай:** \(O(n^2)\)  
- **Пространственная сложность:** \(O(1)\) (сортировка на месте)  

**2) Худший случай**  
- Массив, где элементы расположены в обратном порядке (например, `[5,4,3,2,1]`), особенно при неудачном выборе шага.  

**3) Доказательство сложности**  
- Использует уменьшающийся шаг сравнения, начиная с большого (обычно \(n/1.3\))  
- На каждой итерации шаг уменьшается, пока не станет равным 1 (превращается в Bubble Sort)  
- Эффективность зависит от выбора коэффициента уменьшения шага (1.3 — оптимально)  

**4) Принцип работы**  
1. Выбирается начальный шаг (обычно длина массива, делённая на 1.3)  
2. Сравниваются элементы на расстоянии шага, при необходимости меняются местами  
3. Шаг уменьшается на каждой итерации  
4. Когда шаг становится равным 1, выполняется финальный проход как в Bubble Sort  

**5) Пошаговый пример**  
**Массив:** [8, 4, 1, 3, 2, 5, 7, 6]  
1. Начальный шаг: 8/1.3 ≈ 6:  
   - Сравниваем пары с шагом 6: (8,5), (4,7), (1,6) → меняем 8 и 5 → [5,4,1,3,2,8,7,6]  
2. Новый шаг: 6/1.3 ≈ 4:  
   - Сравниваем пары с шагом 4: (5,2), (4,8), (1,7), (3,6) → меняем 4 и 8 → [5,2,1,3,4,7,6,8]  
3. Шаг уменьшается до 3:  
   - (5,3), (2,4), (1,7), (3,6) → меняем 5 и 3 → [3,2,1,5,4,7,6,8]  
4. Шаг уменьшается до 2:  
   - (3,1), (2,5), (1,4), (5,7), (4,6) → меняем 3 и 1, 2 и 1 → [1,2,3,5,4,7,6,8]  
5. Финальный проход (шаг=1, Bubble Sort):  
   - [1,2,3,4,5,6,7,8]  

**6) Результат работы**  
Вход: [8,4,1,3,2,5,7,6] → Выход: [1,2,3,4,5,6,7,8]  

**Особенности:**  
- Улучшенная версия Bubble Sort  
- Эффективен для средних по размеру массивов  
- Используется в некоторых встроенных системах из-за простоты реализации  

## **Heap Sort**

#### **1) Временная сложность**  
- **Все случаи (худший, средний, лучший):** \(O(n \log n)\)  
- **Пространственная сложность:** \(O(1)\) (сортировка на месте)  

---  

#### **2) Худший случай**  
Любой неотсортированный массив, так как сложность всегда \(O(n \log n)\).  

---  

#### **3) Доказательство сложности**  
1. **Построение кучи:** \(O(n)\)  
   - Превращение массива в кучу выполняется за линейное время.  
2. **Извлечение элементов:** \(O(\log n)\) на каждый из \(n\) элементов → \(O(n \log n)\)  
   - Каждое извлечение максимума требует "просеивания" (heapify) за \(O(\log n)\).  

**Итого:** \(O(n) + O(n \log n) = O(n \log n)\).  

---  

#### **4) Принцип работы**  
1. **Построение max-кучи** из массива:  
   - Родитель ≥ потомков для каждого узла.  
2. **Сортировка:**  
   - Максимум (корень) меняется с последним элементом кучи.  
   - Уменьшаем размер кучи на 1 и выполняем `heapify` для нового корня.  
   - Повторяем, пока куча не опустеет.  

---  

#### **5) Пошаговое выполнение**  
**Пример:** `[3, 1, 4, 2]`  
1. **Построение кучи:**  
   - Исходный массив: `[3, 1, 4, 2]`  
   - Просеиваем вниз, начиная с последнего родителя (индекс 1):  
     - Узел 1 (`1`): потомки `3`, `4` → меняем `1` и `4` → `[3, 4, 1, 2]`  
     - Узел 0 (`3`): потомки `4`, `1` → меняем `3` и `4` → `[4, 3, 1, 2]`  
   - Куча: `[4, 3, 1, 2]`  
2. **Извлечение элементов:**  
   - Шаг 1: Меняем `4` (корень) и `2` (последний) → `[2, 3, 1, 4]`, размер кучи = 3.  
     - Просеиваем `2`: меняем с `3` → `[3, 2, 1, 4]`.  
   - Шаг 2: Меняем `3` и `1` → `[1, 2, 3, 4]`, размер кучи = 2.  
     - Просеиваем `1`: условие кучи выполнено.  
   - Шаг 3: Меняем `2` и `1` → `[1, 2, 3, 4]`, сортировка завершена.  

---  

#### **6) Результат работы**  
- **Вход:** `[3, 1, 4, 2]`  
- **Выход:** `[1, 2, 3, 4]`  

---  

#### **Особенности:**  
- Не требует дополнительной памяти (в отличие от Merge Sort).  
- Неустойчив (меняет порядок равных элементов).  
- Используется в Linux-ядре для сортировки указателей.  

## **Smooth Sort**

#### **1) Временная сложность**  
- **Лучший случай:** \(O(n)\) (уже отсортированный массив)  
- **Средний случай:** \(O(n \log n)\)  
- **Худший случай:** \(O(n \log n)\)  
- **Пространственная сложность:** \(O(1)\)  

#### **2) Худший случай**  
Обратно отсортированный массив (например, `[4, 3, 2, 1]`), где требуется максимальное количество перестановок.  

#### **3) Доказательство сложности**  
- Основан на **леонардовых кучах** (специальная структура данных, близкая к кучам).  
- В худшем случае ведёт себя как Heap Sort (\(O(n \log n)\)), но на частично упорядоченных данных работает быстрее.  

#### **4) Принцип работы**  
1. **Построение леонардовых куч** в массиве:  
   - Использует последовательность чисел Леонардо (1, 1, 3, 5, 9, 15, ...).  
   - Каждая куча соответствует одному из чисел Леонардо.  
2. **Балансировка куч** для поддержания порядка.  
3. **Последовательное извлечение максимумов** (аналогично Heap Sort).  

#### **5) Пошаговый пример**  
**Массив:** `[3, 1, 4, 2]`  
1. **Разбиение на кучи:**  
   - Размеры куч подбираются по числам Леонардо.  
   - Для массива из 4 элементов: куча размером 3 + 1.  
2. **Упорядочивание куч:**  
   - Сначала строится куча из 3 элементов: `[3, 1, 4]` → упорядочивается как `[4, 1, 3]`.  
   - Затем добавляется одиночный элемент `[2]`.  
3. **Извлечение элементов:**  
   - Извлекается максимум `4` → массив `[3, 1, 2]`.  
   - Повторяется процесс для оставшихся элементов.  

#### **6) Результат работы**  
- **Вход:** `[3, 1, 4, 2]`  
- **Выход:** `[1, 2, 3, 4]`  

#### **Особенности:**  
- Разработан Э. Дейкстрой как улучшение Heap Sort.  
- Эффективен на частично упорядоченных данных.  
- Сложен в реализации из-за работы с числами Леонардо.  

## **Quick Sort**

#### **1) Временная сложность**
- **Лучший случай:** O(n log n) (при равномерном разбиении)
- **Средний случай:** O(n log n)
- **Худший случай:** O(n²) (при неудачных опорных элементах)
- **Пространственная сложность:** O(log n) (из-за рекурсии)

#### **2) Худший случай**
Уже отсортированный или обратно отсортированный массив при выборе первого/последнего элемента в качестве опоры (например, [1,2,3,4] или [4,3,2,1])

#### **3) Доказательство сложности**
- В лучшем случае массив делится пополам: T(n) = 2T(n/2) + O(n) → O(n log n)
- В худшем случае массив делится на 1 и n-1: T(n) = T(n-1) + O(n) → O(n²)
- Средний случай: математическое ожидание дает O(n log n)

#### **4) Принцип работы**
1. Выбирается опорный элемент (pivot)
2. Массив разбивается на три части:
   - Элементы < pivot
   - Элементы = pivot
   - Элементы > pivot
3. Рекурсивно сортируются подмассивы

#### **5) Пошаговый пример**
**Массив:** [3, 1, 4, 2]
1. Выбираем опору (pivot = 3)
2. Разбиваем:
   - Меньшие: [1, 2]
   - Равные: [3]
   - Большие: [4]
3. Рекурсивно сортируем [1, 2]:
   - Pivot = 1 → [1], [2]
4. Объединяем: [1, 2] + [3] + [4] → [1, 2, 3, 4]

#### **6) Результат работы**
- Вход: [3, 1, 4, 2]
- Выход: [1, 2, 3, 4]

#### **Оптимизации**
1. Выбор медианы в качестве опоры
2. Переход на сортировку вставками для малых подмассивов
3. Трехчастное разбиение (Dutch National Flag)

#### **Особенности**
- Один из самых быстрых алгоритмов на практике
- Неустойчивая сортировка
- Используется в стандартных библиотеках C++, Java

## **Intro Sort**

#### **1) Временная сложность**  
- **Все случаи (худший/средний/лучший):** \(O(n \log n)\)  
- **Пространственная сложность:** \(O(\log n)\) (из-за рекурсии в Quick Sort)  

#### **2) Худший случай**  
Любой массив, где Quick Sort деградирует до \(O(n^2)\) (например, уже отсортированный массив при плохом выборе опоры).  

#### **3) Доказательство сложности**  
1. **Гибридный подход:**  
   - Начинает с **Quick Sort** (в среднем \(O(n \log n)\)).  
   - При превышении глубины рекурсии \(2 \log n\) переключается на **Heap Sort** (гарантирует \(O(n \log n)\)).  
   - Для маленьких подмассивов (\(n < 16\)) использует **Insertion Sort** (уменьшает константу).  
2. **Гарантированная сложность:**  
   - Quick Sort не может деградировать, так как есть "страховка" Heap Sort.  

#### **4) Принцип работы**  
1. **Быстрая сортировка:**  
   - Рекурсивно сортирует подмассивы.  
   - Отслеживает глубину рекурсии.  
2. **Проверка глубины:**  
   - Если глубина > \(2 \log n\), переключается на Heap Sort.  
3. **Оптимизация для малых массивов:**  
   - При \(n < 16\) применяет Insertion Sort.  

#### **5) Пошаговый пример**  
**Массив:** `[8, 3, 10, 1, 6, 14, 4, 7, 13]`  
1. **Quick Sort:**  
   - Опорный элемент = 8.  
   - Разбиение: `[3,1,6,4,7]`, `[8]`, `[10,14,13]`.  
2. **Рекурсия:**  
   - Левый подмассив `[3,1,6,4,7]`:  
     - Глубина рекурсии = 1.  
     - Сортировка аналогично.  
3. **Проверка глубины:**  
   - Если глубина превысит \(2 \log 9 \approx 6\), переключится на Heap Sort.  
4. **Финальная сортировка:**  
   - Объединение отсортированных подмассивов.  

#### **6) Результат работы**  
- **Вход:** `[8, 3, 10, 1, 6, 14, 4, 7, 13]`  
- **Выход:** `[1, 3, 4, 6, 7, 8, 10, 13, 14]`  

#### **Особенности**  
- Разработан Дэвидом Мюссером в 1997 году.  
- **Используется в STL (C++)** как стандартный алгоритм `std::sort`.  
- Сочетает скорость Quick Sort и гарантии Heap Sort.  

## **Patience Sort**

#### **1) Временная сложность**
- **Лучший случай:** O(n log n)
- **Худший случай:** O(n log n)
- **Пространственная сложность:** O(n)

#### **2) Худший случай**
Уже отсортированный в обратном порядке массив (например, [5,4,3,2,1])

#### **3) Доказательство сложности**
1. Построение стопок: O(n log n) (бинарный поиск для каждой карты)
2. Объединение стопок: O(n log n) (использование приоритетной очереди)
Итого: O(n log n) в любом случае

#### **4) Принцип работы**
1. Разложить элементы в "стопки" по правилу:
   - Новая карта кладётся на самую левую стопку, где верхняя карта >= текущей
   - Если такой нет - создаётся новая стопка справа
2. Объединить стопки, всегда выбирая минимальный верхний элемент

#### **5) Пошаговый пример**
**Массив:** [3, 1, 4, 2]
1. Создание стопок:
   - 3 → [3]
   - 1 → [3], [1] (1 < 3, новая стопка)
   - 4 → [3], [1], [4] (4 > все верхние)
   - 2 → [3], [1,2], [4] (2 кладём на [1])
2. Объединение:
   - Выбираем минимальный верх: 1 → 2 → 3 → 4

#### **6) Результат работы**
- Вход: [3,1,4,2]
- Выход: [1,2,3,4]

#### **Особенности**
- Основан на карточной игре "Пасьянс"
- Позволяет находить самую длинную возрастающую подпоследовательность
- Использует жадный алгоритм при построении стопок

## **Shell Sort**

#### **1) Временная сложность**
- **Лучший случай:** O(n log n) (для некоторых последовательностей шагов)
- **Средний случай:** Зависит от выбора шагов, обычно O(n^(1.3-2))
- **Худший случай:** O(n²) (для простых последовательностей шагов)
- **Пространственная сложность:** O(1) (сортировка на месте)

#### **2) Худший случай**
Массив, где элементы расположены в обратном порядке при неудачной последовательности шагов (например, [5,4,3,2,1] с шагами 1, 2, 4)

#### **3) Доказательство сложности**
Сложность зависит от выбранной последовательности шагов:
- Последовательность Шелла (n/2, n/4,...): O(n²)
- Последовательность Хиббарда (2^k-1): O(n^(3/2))
- Последовательность Пратта (2^p3^q): O(n log² n)
- Последовательность Седжвика: O(n^(4/3))

#### **4) Принцип работы**
1. Выбирается последовательность шагов (gaps)
2. На каждом шаге массив сортируется вставками элементов, отстоящих друг от друга на текущий шаг
3. Шаг постепенно уменьшается до 1 (когда алгоритм становится обычной сортировкой вставками)

#### **5) Пошаговый пример**
**Массив:** [5, 3, 8, 2, 1, 4]
**Последовательность шагов:** 3, 1

1. Шаг 3:
   - Сортируем подмассивы с шагом 3:
     [5,2] → [2,5]
     [3,1] → [1,3]
     [8,4] → [4,8]
   - Результат: [2, 1, 4, 5, 3, 8]

2. Шаг 1 (обычная сортировка вставками):
   - Полностью сортируем массив: [1, 2, 3, 4, 5, 8]

#### **6) Результат работы**
- Вход: [5,3,8,2,1,4]
- Выход: [1,2,3,4,5,8]

#### **Особенности**
- Является улучшенной версией сортировки вставками
- Эффективен для средних по размеру массивов
- Неустойчивая сортировка
- Широко использовался в ранних UNIX-системах

## **Radix Sort**

#### **1) Временная сложность**
- **Все случаи:** O(nk), где k - количество разрядов
- **Пространственная сложность:** O(n + k)

#### **2) Худший случай**
Все элементы имеют одинаковое количество разрядов (например, [111, 222, 333])

#### **3) Доказательство сложности**
1. Для каждого из k разрядов:
   - Распределение по корзинам: O(n)
   - Сбор из корзин: O(n)
2. Итого: O(k*(n + n)) = O(nk)
3. Если k = O(log n), то сложность O(n log n)

#### **4) Принцип работы**
1. Определить максимальное количество разрядов
2. Сортировать от младшего разряда к старшему:
   - Распределить элементы по корзинам (0-9)
   - Собрать элементы в порядке корзин
3. Повторять для каждого разряда

#### **5) Пошаговый пример (LSD)**
**Массив:** [170, 45, 75, 90, 802, 24, 2, 66]

1. По младшему разряду:
   - Корзины: 
     0: [90, 170]
     2: [802, 2]
     4: [24]
     5: [45, 75]
     6: [66]
   - Результат: [90,170,802,2,24,45,75,66]

2. По среднему разряду:
   - Корзины:
     0: [802,2]
     1: [170]
     2: [24]
     4: [45]
     6: [66]
     7: [75,90]
   - Результат: [802,2,170,24,45,66,75,90]

3. По старшему разряду:
   - Корзины:
     0: [2,24,45,66,75,90]
     1: [170]
     8: [802]
   - Результат: [2,24,45,66,75,90,170,802]

#### **6) Результат работы**
- Вход: [170,45,75,90,802,24,2,66]
- Выход: [2,24,45,66,75,90,170,802]

#### **Особенности**
- Работает только с числами или строками фиксированной длины
- Может быть LSD (младшие разряды сначала) или MSD (старшие разряды сначала)
- Используется в сортировке больших целых чисел
- Устойчивая сортировка

## **Topological Sort**

#### **1) Временная сложность**  
- **Все случаи:** \(O(V + E)\) (вершины + рёбра)  
- **Пространственная сложность:** \(O(V)\) (для хранения стека/очереди)  

#### **2) Худший случай**  
Граф с максимальным количеством рёбер (полный DAG), где \(E = V(V-1)/2\), но сложность остаётся \(O(V + E)\).  

#### **3) Доказательство сложности**  
1. Обход всех вершин и рёбер: \(O(V + E)\)  
2. Использование стека/очереди: \(O(1)\) на операцию  
3. Итог: линейная зависимость от размера графа  

#### **4) Принцип работы**  
1. **Выбор вершины с нулевой входящей степенью** (нет зависимостей)  
2. **Добавление в результат** и удаление её рёбер из графа  
3. **Повтор** до обработки всех вершин  
4. Если остались вершины → граф имеет цикл (не DAG)  

#### **5) Пошаговый пример**  
**Граф задач:**  
- B зависит от A  
- C зависит от B  
- D зависит от A  

1. Находим вершины без входящих рёбер: [A]  
2. Обрабатываем A: результат = [A], удаляем рёбра A→B, A→D  
3. Теперь вершины без зависимостей: [B, D]  
4. Обрабатываем B: результат = [A, B], удаляем B→C  
5. Оставшиеся: [D, C]  
6. Обрабатываем D: результат = [A, B, D]  
7. Обрабатываем C: результат = [A, B, D, C]  

#### **6) Результат работы**  
- **Допустимый порядок:** [A, B, D, C] или [A, D, B, C]  
- **Не DAG пример:** Если добавить C→A, топсортировка невозможна (есть цикл)  

#### **7) Особенности**  
- **Применение:**  
  - Планирование задач  
  - Компиляция (разрешение зависимостей)  
  - Установка ПО с зависимостями  
- **Алгоритмы:**  
  - **Kahn's algorithm** (удаление вершин с нулевой входящей степенью)  
  - **DFS-based** (постобработка вершин)  

---  
# **Find The Pattern**

## **Алгоритм Кнута-Морриса-Пратта (KMP)**

#### **1) Временная сложность**
- **Препроцессинг:** O(m), где m - длина образца
- **Поиск:** O(n), где n - длина текста
- **Общая сложность:** O(n + m)
- **Пространственная сложность:** O(m) (для хранения префикс-функции)

#### **2) Худший случай**
Текст и образец вида "aaaaa...a", где требуется максимальное количество сравнений

#### **3) Доказательство сложности**
1. Префикс-функция строится за O(m) (каждый символ обрабатывается константное число раз)
2. Поиск выполняется за O(n) (каждый символ текста обрабатывается не более двух раз)
3. Итого: линейная сложность O(n + m)

#### **4) Принцип работы**
1. **Препроцессинг:**
   - Вычисление префикс-функции для образца
   - Префикс-функция показывает длину наибольшего собственного суффикса, совпадающего с префиксом
2. **Поиск:**
   - Посимвольное сравнение текста с образцом
   - При несовпадении используется префикс-функция для определения нового положения сравнения

#### **5) Пошаговый пример**
**Текст:** "ABABDABACDABABCABAB"
**Образец:** "ABABCABAB"

1. Вычисляем префикс-функцию для образца:
   - "A": 0
   - "AB": 0
   - "ABA": 1
   - "ABAB": 2
   - "ABABC": 0
   - "ABABCA": 1
   - "ABABCAB": 2
   - "ABABCABA": 3
   - "ABABCABAB": 4

2. Поиск в тексте:
   - Находим полное совпадение на позиции 10

#### **6) Результат работы**
- Найден образец "ABABCABAB" в тексте, начиная с позиции 10 (индексация с 0)

#### **7) Особенности**
- Эффективен для поиска повторяющихся паттернов
- Не возвращается к уже проверенным символам текста
- Используется в биоинформатике для поиска последовательностей ДНК
- Легко адаптируется для поиска всех вхождений образца

**Следующий алгоритм?** (Далее — **Алгоритм Бойера-Мура**)


## **Алгоритм Бойера-Мура**

#### **1) Временная сложность**
- **Лучший случай:** O(n/m) (например, когда символ из образца отсутствует в тексте)
- **Худший случай:** O(n*m) (например, повторяющиеся паттерны "aaa...")
- **Средний случай:** O(n) (на практике часто линейный)
- **Пространственная сложность:** O(m + k), где k - размер алфавита

#### **2) Худший случай**
Текст: "aaaaaaaa..." и образец: "baaa..." (требуется полный перебор)

#### **3) Доказательство сложности**
1. Использует две эвристики:
   - **Плохой символ** (bad character) - позволяет прыгать на длину образца
   - **Хороший суффикс** (good suffix) - дополнительное ускорение
2. В лучшем случае делает всего O(n/m) сравнений
3. В худшем случае вырождается в наивный алгоритм

#### **4) Принцип работы**
1. **Препроцессинг:**
   - Таблица "плохого символа" (последнее вхождение каждого символа в образце)
   - Таблица "хорошего суффикса" (совпадающие части образца)
2. **Поиск:**
   - Сравнение справа налево
   - При несовпадении - максимальный сдвиг по двум таблицам

#### **5) Пошаговый пример**
**Текст:** "HERE IS A SIMPLE EXAMPLE"  
**Образец:** "EXAMPLE"

1. Препроцессинг:
   - Таблица плохих символов: E→6, X→5, A→4, M→3, P→2, L→1
   - Таблица хороших суффиксов: [..., shift=2 для префикса "E"]

2. Поиск:
   - Первое сравнение: "HERE IS A SIMPLE EXAMPLE"
                          "EXAMPLE" (не совпадает 'P' и ' ')
   - Сдвигаем на 7 (по ' ' нет в образце)
   - Следующая позиция: "EXAMPLE" - полное совпадение

#### **6) Результат работы**
- Найден образец "EXAMPLE" на позиции 17 (индексация с 0)

#### **7) Особенности**
- Самый быстрый на практике для больших алфавитов (английский, Unicode)
- Используется в grep, текстовых редакторах
- Вариации: Бойер-Мур-Хорспул (упрощенная версия)

## **Алгоритм Рабина-Карпа**  

#### **1) Временная сложность**  
- **Лучший/средний случай:** \(O(n + m)\), где \(n\) — длина текста, \(m\) — длина образца  
- **Худший случай:** \(O(n \cdot m)\) (при частых коллизиях хешей)  
- **Пространственная сложность:** \(O(1)\) (если не хранить все позиции вхождений)  

#### **2) Худший случай**  
Текст: "AAAA...AAA", образец: "AAA" (все подстроки имеют одинаковый хеш → проверка всех позиций)  

#### **3) Доказательство сложности**  
1. **Препроцессинг:**  
   - Вычисление хеша образца: \(O(m)\)  
   - Вычисление первого хеша для текста: \(O(m)\)  
2. **Поиск:**  
   - Обновление хеша для каждого сдвига: \(O(1)\) (скользящее хеширование)  
   - В худшем случае (коллизии) — проверка всех символов: \(O(n \cdot m)\)  

#### **4) Принцип работы**  
1. **Хеширование:**  
   - Использует **скользящий хеш** (например, полиномиальный).  
   - Сравнивает хеш образца с хешами подстрок текста.  
2. **Проверка:**  
   - При совпадении хешей — проверка символов (чтобы избежать ложных срабатываний).  

#### **5) Пошаговый пример**  
**Текст:** "ABABDABACDABABCABAB"  
**Образец:** "ABAB"  
**Простое хеширование (сумма кодов):**  

1. Хеш образца: 'A' + 'B' + 'A' + 'B' = 65 + 66 + 65 + 66 = **262**  
2. Вычисляем хеши подстрок текста:  
   - "ABAB" → 262 (совпадение, проверяем символы — да)  
   - "BABD" → 263 → пропускаем  
   - "ABDA" → 264 → пропускаем  
   - ...  
   - Найдены вхождения на позициях 0 и 13.  

#### **6) Результат работы**  
- Найдены вхождения "ABAB" на позициях: **0** и **13**.  

#### **7) Особенности**  
- **Плюсы:**  
  - Прост в реализации.  
  - Эффективен для поиска множества образцов (одновременно).  
- **Минусы:**  
  - Требует хорошей хеш-функции для минимизации коллизий.  
- **Применение:**  
  - Поиск плагиата в текстах.  
  - Детекция вредоносного ПО (сигнатурный поиск).  

# **Graphs**

## **Поиск в ширину (BFS, Breadth-First Search)**

#### **1) Временная сложность**
- **Для матрицы смежности:** O(V²)
- **Для списка смежности:** O(V + E)
- **Пространственная сложность:** O(V)

#### **2) Худший случай**
Полносвязный граф, где необходимо посетить все вершины и ребра

#### **3) Доказательство сложности**
1. Каждая вершина посещается ровно один раз - O(V)
2. Каждое ребро просматривается один раз - O(E)
3. Использование очереди требует O(V) памяти

#### **4) Принцип работы**
1. Начинаем с исходной вершины, добавляем ее в очередь
2. Пока очередь не пуста:
   a) Извлекаем вершину из начала очереди
   b) Добавляем всех ее непосещенных соседей в конец очереди
3. Повторяем до полного обхода графа

#### **5) Пошаговый пример**
**Граф:**
```
    A
   / \
  B   C
 / \   \
D   E   F
```

**Шаги выполнения:**
1. Очередь: [A], Посещены: A
2. Очередь: [B, C], Посещены: A, B, C
3. Очередь: [C, D, E], Посещены: A, B, C, D, E
4. Очередь: [D, E, F], Посещены: A, B, C, D, E, F
5. Очередь: [E, F], Посещены: все
6. Очередь: [F], Посещены: все
7. Очередь: [], Завершение

#### **6) Результат работы**
- Порядок обхода: A, B, C, D, E, F
- Кратчайшие пути от вершины A до всех остальных

#### **7) Особенности**
- Находит кратчайшие пути в невзвешенных графах
- Использует очередь (FIFO)
- Применяется для:
  - Поиска кратчайшего пути
  - Обнаружения связных компонент
  - Тестирования двудольности графа
  - Поиска в соцсетях (расширение круга знакомств)

## **Обход в глубину (DFS, Depth-First Search)**

#### **1) Временная сложность**
- **Для матрицы смежности:** O(V²)
- **Для списка смежности:** O(V + E)
- **Пространственная сложность:** O(V) (из-за стека вызовов)

#### **2) Худший случай**
Граф в виде линейной цепочки вершин (например, A-B-C-D-E-F)

#### **3) Доказательство сложности**
1. Каждая вершина посещается один раз - O(V)
2. Каждое ребро просматривается один раз - O(E)
3. Рекурсивная реализация требует O(V) памяти для стека вызовов

#### **4) Принцип работы**
1. Начинаем с исходной вершины, помечаем как посещенную
2. Для каждой смежной непосещенной вершины:
   a) Рекурсивно применяем DFS
3. Альтернативная реализация использует явный стек

#### **5) Пошаговый пример (рекурсивный)**
**Граф:**
```
    A
   / \
  B   D
 / \   \
C   E   F
```

**Порядок обхода:**
1. Посещаем A
2. Переходим к B (сосед A)
3. Переходим к C (сосед B)
4. Возвращаемся к B, переходим к E
5. Возвращаемся к B, затем к A
6. Переходим к D (сосед A)
7. Переходим к F (сосед D)

#### **6) Результат работы**
- Порядок обхода: A → B → C → E → D → F
- Дерево обхода (без обратных ребер)

#### **7) Особенности**
- **Преимущества:**
  - Экономия памяти при использовании рекурсии
  - Позволяет обнаруживать циклы
  - Используется для топологической сортировки
- **Недостатки:**
  - Не находит кратчайшие пути
  - Может зациклиться при наличии циклов без отметки посещенных вершин
- **Применение:**
  - Поиск компонент связности
  - Проверка на двудольность
  - Поиск мостов и точек сочленения
  - Генерация лабиринтов

## **Алгоритм Флойда-Уоршелла (Floyd-Warshall)**

#### **1) Временная сложность**
- **Все случаи:** O(V³)
- **Пространственная сложность:** O(V²)

#### **2) Худший случай**
Любой граф, так как алгоритм всегда обрабатывает все возможные тройки вершин

#### **3) Доказательство сложности**
1. Три вложенных цикла по количеству вершин (V)
2. Каждая итерация выполняется за константное время
3. Итого: V × V × V = O(V³)

#### **4) Принцип работы**
1. Инициализация матрицы расстояний:
   - 0 для диагонали
   - Вес ребра для смежных вершин
   - ∞ для остальных пар
2. Для каждой промежуточной вершины k:
   - Для каждой пары вершин i и j:
     - Если путь i→k→j короче текущего i→j, обновляем расстояние

#### **5) Пошаговый пример**
**Граф:**
```
        (1)
      A ---> B
      |    / ^
    (4)|  /  |(2)
      v /    |
      C <--- D
        (3)
```

**Итерации:**
1. Без промежуточных вершин: начальная матрица
2. С промежуточной A: обновляем B→C через A (1+4=5)
3. С промежуточной B: обновляем A→C через B (1+2=3)
4. С промежуточной C: обновляем D→B через C (3+2=5)
5. С промежуточной D: обновлений нет

#### **6) Результат работы**
**Матрица кратчайших путей:**
```
   A  B  C  D
A  0  1  3  5
B ∞  0  2  4
C ∞  5  0  3
D ∞  2  5  0
```

#### **7) Особенности**
- Находит кратчайшие пути между всеми парами вершин
- Работает с отрицательными весами (но не с отрицательными циклами)
- Может обнаруживать наличие отрицательных циклов
- Используется в:
  - Анализе транспортных сетей
  - Маршрутизации в сетях
  - Оптимизации логистических цепочек

## **Алгоритм Беллмана-Форда**

#### **1) Временная сложность**
- **Все случаи:** O(V×E)
- **Пространственная сложность:** O(V)

#### **2) Худший случай**
Граф, где необходимо проверить все ребра на всех итерациях (например, линейная цепочка вершин с отрицательными весами)

#### **3) Доказательство сложности**
1. Выполняется V-1 итераций
2. На каждой итерации проверяются все E ребер
3. Итого: (V-1)×E = O(V×E)

#### **4) Принцип работы**
1. Инициализация расстояний:
   - 0 для стартовой вершины
   - ∞ для остальных
2. Релаксация всех ребер (V-1 раз):
   - Если расстояние до v > (расстояние до u + вес ребра u→v), обновляем
3. Проверка на отрицательные циклы:
   - Если на V-й итерации происходит обновление - цикл есть

#### **5) Пошаговый пример**
**Граф:**
```
S --3→ A
|     /  \
4    1   -2
↓ /     ↓
B ←2-- C
```

**Итерации:**
1. Инициализация: S=0, A=B=C=∞
2. 1-я итерация:
   - S→A: A=3
   - S→B: B=4
   - A→C: C=2
   - B→A: A=min(3,4+1)=3
   - C→B: B=min(4,2+2)=4
3. 2-я итерация:
   - Обновляется C=min(2,3+1)=2
4. Проверка на 3-й итерации - изменений нет, циклов не обнаружено

#### **6) Результат работы**
**Кратчайшие пути от S:**
- S→A: 3
- S→B: 4
- S→C: 2

#### **7) Особенности**
- **Преимущества:**
  - Работает с отрицательными весами
  - Обнаруживает отрицательные циклы
- **Недостатки:**
  - Медленнее Дейкстры для положительных весов
- **Применение:**
  - Финансовый анализ (арбитражные ситуации)
  - Маршрутизация в сетях с возможными отрицательными метриками
  - Расчет минимальных затрат в логистике

## **Алгоритм Левита (Levite's Algorithm)**

#### **1) Временная сложность**
- **Лучший случай:** O(E + V)
- **Худший случай:** O(V²×E)
- **Средний случай:** O(V×E)
- **Пространственная сложность:** O(V)

#### **2) Худший случай**
Граф, где вершины постоянно переходят между обычной и срочной очередями (например, граф с отрицательными ребрами, создающими "гонку" расстояний)

#### **3) Доказательство сложности**
1. Использует три очереди с разными приоритетами
2. В худшем случае вершина может обрабатываться до V раз
3. Каждая обработка вершины требует проверки всех E/V рёбер в среднем
4. Итого: V × (E/V) × V = O(V²×E)

#### **4) Принцип работы**
1. Инициализация:
   - Обычная очередь (M0)
   - Срочная очередь (M1)
   - Очередь обработанных (M2)
   - Расстояния: 0 для стартовой, ∞ для остальных

2. Основной цикл:
   - Обработка вершин из M1 (высший приоритет)
   - Затем из M0
   - При обновлении расстояния вершина переходит в M1
   - После обработки вершина попадает в M2

#### **5) Пошаговый пример**
**Граф:**
```
A --3→ B
|    /  \
2   -1   4
↓ /    ↓
C ←1-- D
```

**Шаги:**
1. Инициализация: A=0, B=C=D=∞
2. A обрабатывается:
   - B=3 (M1)
   - C=2 (M1)
3. Обработка M1:
   - B: D=3+4=7 (M0)
   - C: B=min(3,2-1)=1 (B переходит в M1)
4. Повторная обработка B:
   - D=min(7,1+4)=5 (D в M1)
5. Результат: A=0, B=1, C=2, D=5

#### **6) Результат работы**
**Кратчайшие пути от A:**
- A→B: 1 (A→C→B)
- A→C: 2
- A→D: 5 (A→C→B→D)

#### **7) Особенности**
- **Преимущества:**
  - Эффективен для графов с отрицательными рёбрами
  - Часто работает быстрее Беллмана-Форда на практике
- **Недостатки:**
  - Сложная реализация
  - Непредсказуемое время выполнения
- **Применение:**
  - Транспортная логистика с переменными тарифами
  - Финансовые расчеты с возможными бонусами/штрафами

## **Алгоритм Дейкстры: Кратчайший путь в графе**  

#### **1) Выбор временной сложности алгоритма**  
Временная сложность алгоритма Дейкстры зависит от реализации:  
- **Наивная реализация (массив):** \(O(|V|^2)\), где \(|V|\) — количество вершин.  
- **С кучей (приоритетной очередью):**  
  - **Бинарная куча:** \(O((|V| + |E|) \log |V|)\)  
  - **Фибоначчиева куча:** \(O(|V| \log |V| + |E|)\)  

#### **2) Выбор худшего случая для алгоритма**  
Худший случай — это полный граф, где каждые две вершины соединены ребром (\(|E| = O(|V|^2)\)).  
- Для реализации с массивом: \(O(|V|^2)\) (как и в среднем).  
- Для реализации с кучей: \(O(|V|^2 \log |V|)\) (так как \(|E| \approx |V|^2\)).  

#### **3) Доказательство сложности алгоритма**  
**Наивная реализация (массив):**  
- На каждом шаге выбираем вершину с минимальным расстоянием (\(O(|V|)\)).  
- Делаем это \(|V|\) раз → \(O(|V|^2)\).  
- Обновляем расстояния для всех соседей (\(O(|E|)\) в сумме).  
- Итог: \(O(|V|^2 + |E|) = O(|V|^2)\) (если \(|E| \leq |V|^2\)).  

**С кучей:**  
- Извлечение минимума — \(O(\log |V|)\), выполняется \(|V|\) раз → \(O(|V| \log |V|)\).  
- Уменьшение ключа (релаксация) — \(O(\log |V|)\), выполняется \(|E|\) раз → \(O(|E| \log |V|)\).  
- Итог: \(O((|V| + |E|) \log |V|)\).  

#### **4) Принцип работы алгоритма**  
Алгоритм Дейкстры находит кратчайшие пути от начальной вершины до всех остальных в **взвешенном графе без отрицательных рёбер**.  
1) Инициализация: расстояние до стартовой вершины = 0, до остальных = ∞.  
2) На каждом шаге выбирается вершина с минимальным текущим расстоянием.  
3) Для всех её соседей выполняется **релаксация** (обновление расстояния, если найден более короткий путь).  
4) Процесс повторяется, пока не будут обработаны все вершины.  

#### **5) Пошаговое выполнение алгоритма**  
**Граф:**  
```
A --1-- B --3-- D  
 \     / \     /  
  4   2   1   5  
   \ /     \ /  
    C --6-- E  
```
**Найти кратчайшие пути из A:**  

| Шаг | Текущая вершина | Расстояния (A, B, C, D, E) | Очередь (вершина: расстояние) |  
|-----|------------------|----------------------------|--------------------------------|  
| 0   | -                | [0, ∞, ∞, ∞, ∞]            | A:0, B:∞, C:∞, D:∞, E:∞        |  
| 1   | A (0)            | [0, 1, 4, ∞, ∞]            | B:1, C:4                       |  
| 2   | B (1)            | [0, 1, 3, 4, 2]            | C:3, D:4, E:2                  |  
| 3   | E (2)            | [0, 1, 3, 4, 2]            | C:3, D:4                       |  
| 4   | C (3)            | [0, 1, 3, 4, 2]            | D:4                            |  
| 5   | D (4)            | [0, 1, 3, 4, 2]            | -                              |  

**Итоговые расстояния:**  
- A → A: 0  
- A → B: 1  
- A → C: 3  
- A → D: 4  
- A → E: 2  

#### **6) Результат работы алгоритма**  
Алгоритм возвращает **массив кратчайших расстояний** от начальной вершины до всех остальных.  
Для примера:  
```python
dist = {'A': 0, 'B': 1, 'C': 3, 'D': 4, 'E': 2}
```  
Если нужно восстановить сам путь, можно хранить **предков** для каждой вершины.  

### **Вывод**  
Алгоритм Дейкстры эффективно решает задачу поиска кратчайших путей в графах с **неотрицательными весами**, используя **жадную стратегию**. Его сложность можно оптимизировать с помощью структур данных, таких как **куча**.

## **Алгоритм Краскала: Построение минимального остовного дерева (МОД)**  

#### **1) Временная сложность алгоритма**  
- Сортировка рёбер: \(O(|E| \log |E|)\).  
- Проверка на цикл с **системой непересекающихся множеств (DSU)**:  
  - \(O(\alpha(|V|))\) на операцию, где \(\alpha\) — функция Аккермана (почти константа).  
  - Всего \(O(|E| \cdot \alpha(|V|))\).  
- **Итоговая сложность:** \(O(|E| \log |E|)\) или \(O(|E| \log |V|)\) (так как \(|E| \leq |V|^2\)).  

#### **2) Худший случай для алгоритма**  
- **Полный граф** (\(|E| = O(|V|^2)\)), так как сортировка рёбер займёт \(O(|V|^2 \log |V|)\).  
- Если рёбра уже отсортированы, DSU оптимизирует процесс до \(O(|E| \cdot \alpha(|V|))\).  

#### **3) Доказательство сложности**  
- Сортировка рёбер доминирует в сложности (\(O(|E| \log |E|)\)).  
- DSU с эвристиками (ранг + сжатие пути) делает операции **Union** и **Find** почти за \(O(1)\).  
- Алгоритм перебирает все рёбра (\(O(|E|)\)) и для каждого делает 2 операции **Find** + 1 **Union** → \(O(|E| \cdot \alpha(|V|))\).  

#### **4) Принцип работы алгоритма**  
Алгоритм Краскала строит **минимальное остовное дерево** (подграф без циклов с минимальным суммарным весом):  
1) **Сортировка всех рёбер** по весу (от меньшего к большему).  
2) **Пошаговое добавление рёбер** в дерево, если они **не образуют цикл**.  
3) Проверка на цикл выполняется с помощью **DSU (Disjoint Set Union)**.  
4) Процесс продолжается, пока не будут добавлены \(|V| - 1\) рёбер.  

#### **5) Пошаговое выполнение алгоритма**  
**Граф:**  
```
A --1-- B --3-- D  
 \     / \     /  
  4   2   1   5  
   \ /     \ /  
    C --6-- E  
```
**Рёбра:** (A-B:1), (B-E:1), (B-C:2), (A-C:4), (B-D:3), (C-E:6), (D-E:5)  

| Шаг | Выбранное ребро | Проверка цикла (DSU) | Добавлено? | Остовное дерево |  
|-----|------------------|----------------------|------------|------------------|  
| 1   | A-B (1)          | A и B в разных множествах | Да        | A-B              |  
| 2   | B-E (1)          | B и E в разных множествах | Да        | A-B, B-E         |  
| 3   | B-C (2)          | B и C в разных множествах | Да        | A-B, B-E, B-C    |  
| 4   | B-D (3)          | B и D в разных множествах | Да        | A-B, B-E, B-C, B-D |  
| 5   | D-E (5)          | D и E в одном множестве (через B) | Нет     | Не добавляется   |  
| 6   | A-C (4)          | A и C в одном множестве (через B) | Нет     | Не добавляется   |  
| 7   | C-E (6)          | C и E в одном множестве (через B) | Нет     | Не добавляется   |  

**Итоговое МОД:**  
```
A --1-- B --1-- E  
       / \  
      2   3  
     /     \  
    C       D  
```
**Суммарный вес = 1 + 1 + 2 + 3 = 7**  

#### **6) Результат работы алгоритма**  
- Алгоритм возвращает **множество рёбер**, образующих минимальное остовное дерево.  
- Для примера:  
  - Рёбра: `{(A-B, 1), (B-E, 1), (B-C, 2), (B-D, 3)}`  
  - Вес: **7**  

### **Вывод**  
Алгоритм Краскала эффективно строит МОД, используя **жадную стратегию** (выбор наименьших рёбер) и **DSU** для проверки циклов. Его сложность определяется сортировкой рёбер (\(O(|E| \log |E|)\)), что делает его удобным для **разреженных графов**.

## **Алгоритм Прима мин остовного дерева **

### 1) **Выбор временной сложности алгоритма**  
Временная сложность алгоритма Прима зависит от реализации:  
- **Наивная реализация** (с использованием массива или списка для поиска минимального ребра): **O(V²)**, где V — количество вершин.  
- **С использованием приоритетной очереди (кучи)**: **O(E + V log V)**, где E — количество рёбер.  

Если используется **фибоначчиева куча**, сложность можно улучшить до **O(E + V log V)**.

---

### 2) **Выбор худшего случая для алгоритма**  
Худший случай возникает в **плотном графе**, где количество рёбер **E ≈ V²** (например, полный граф).  
В этом случае:  
- Наивная реализация: **O(V²)**  
- С кучей: **O(V² log V)** (так как E = V²)  

---

### 3) **Доказательство сложности алгоритма**  
**Основные шаги алгоритма:**  
1. Инициализация: выбор стартовой вершины — **O(1)**.  
2. Основной цикл выполняется **V раз** (добавляем все вершины).  
3. На каждом шаге:  
   - Наивный поиск минимального ребра — **O(V)** → **O(V²)** в сумме.  
   - С кучей: извлечение минимума — **O(log V)**, обновление ключей — **O(E)** в сумме (так как каждое ребро обрабатывается один раз).  

Итоговая сложность:  
- **O(V²)** (наивный)  
- **O(E + V log V)** (с кучей)  

---

### 4) **Принцип работы алгоритма**  
Алгоритм Прима строит минимальное остовное дерево (МОД), **последовательно добавляя вершины** к дереву, выбирая на каждом шаге **минимальное ребро**, соединяющее уже включённые вершины с оставшимися.  

**Ключевые моменты:**  
- Начинаем с произвольной вершины.  
- Поддерживаем множество вершин, уже включённых в дерево.  
- На каждом шаге выбираем ребро с **минимальным весом**, ведущее к новой вершине.  

---

### 5) **Пошаговое выполнение алгоритма**  
**Пример:** Граф с вершинами A, B, C, D и рёбрами:  
- AB: 1  
- AC: 3  
- AD: 4  
- BC: 2  
- BD: 5  

**Шаги:**  
1. **Старт:** Выбираем вершину **A**.  
2. **Шаг 1:** Минимальное ребро из A — AB (вес 1). Добавляем B.  
3. **Шаг 2:** Теперь рассматриваем рёбра из A и B:  
   - AC (3), AD (4), BC (2).  
   - Минимальное — BC (2). Добавляем C.  
4. **Шаг 3:** Осталось ребро AD (4) и BD (5).  
   - Минимальное — AD (4). Добавляем D.  
5. **Завершение:** Все вершины включены.  

**МОД:** AB (1), BC (2), AD (4).  

---

### 6) **Результат работы алгоритма**  
- **Минимальное остовное дерево** (МОД) — подграф, связывающий все вершины с минимальным суммарным весом рёбер.  
- Для примера:  
  - Рёбра: AB (1), BC (2), AD (4).  
  - Общий вес: **1 + 2 + 4 = 7**.  

**Вывод:** Алгоритм Прима эффективно находит МОД, жадным образом выбирая наименьшие рёбра на каждом шаге.

# Построение выпуклой оболочки

## **Алгоритм Грэхема**
### 1) **Выбор временной сложности алгоритма**  
Временная сложность алгоритма Грэхема:  
- **Сортировка точек** по полярному углу: **O(n log n)** (используется быстрая сортировка или сортировка слиянием).  
- **Построение оболочки** с помощью стека: **O(n)** (каждая точка обрабатывается один раз).  

**Итоговая сложность:** **O(n log n)**.  

---

### 2) **Выбор худшего случая для алгоритма**  
Худший случай — когда **все точки уже лежат на выпуклой оболочке**, и алгоритму приходится проверить каждую из них.  
- Сортировка остаётся **O(n log n)**.  
- Построение оболочки — **O(n)**.  

**Итоговая сложность не ухудшается**, но если точек очень много, сортировка становится узким местом.  

---

### 3) **Доказательство сложности алгоритма**  
1. **Сортировка точек** по полярному углу относительно самой нижней левой точки: **O(n log n)**.  
2. **Построение оболочки:**  
   - Каждая точка **добавляется в стек один раз** (O(1) в среднем).  
   - Некоторые точки **удаляются из стека**, но каждая точка удаляется не более одного раза → **O(n)**.  

**Итого:** **O(n log n) + O(n) = O(n log n)**.  

---

### 4) **Принцип работы алгоритма**  
Алгоритм Грэхема строит выпуклую оболочку, используя **метод обхода Грэхема**:  
1. **Находим самую нижнюю левую точку** (она точно входит в оболочку).  
2. **Сортируем остальные точки** по полярному углу относительно неё.  
3. **Последовательно добавляем точки в стек**, проверяя, не образуют ли последние три точки **невыпуклый поворот** (если да — удаляем среднюю).  

**Ключевая идея:**  
- Выпуклая оболочка строится за счёт **отбрасывания "вогнутых" углов**.  
- Используется **стек** для эффективного удаления ненужных точек.  

---

### 5) **Пошаговое выполнение алгоритма**  
**Пример:** Точки A(0,0), B(1,1), C(2,2), D(3,1), E(1,-1).  

1. **Шаг 1:** Находим самую нижнюю левую точку — **E(1,-1)**.  
2. **Шаг 2:** Сортируем остальные по углу относительно E:  
   - A(0,0), B(1,1), D(3,1), C(2,2).  
3. **Шаг 3:** Построение оболочки:  
   - Добавляем **E, A, B** в стек.  
   - Проверяем **B → D**:  
     - Угол ABD — **выпуклый**, оставляем.  
   - Добавляем **D**.  
   - Проверяем **D → C**:  
     - Угол BDC — **вогнутый**, удаляем D, возвращаемся к B.  
     - Угол ABC — **выпуклый**, добавляем C.  
4. **Результат:** Оболочка — **E, A, B, C**.  

---

### 6) **Результат работы алгоритма**  
- **Выпуклая оболочка** — минимальный выпуклый многоугольник, содержащий все точки.  
- Для примера:  
  - Точки оболочки: **E(1,-1), A(0,0), B(1,1), C(2,2)**.  
  - Форма — выпуклая ломаная, соединяющая эти точки.  

**Вывод:** Алгоритм Грэхема эффективно строит выпуклую оболочку за **O(n log n)**, используя сортировку и стек.

## **Алгоритм Джарвиса**

### 1) **Выбор временной сложности алгоритма**  
Алгоритм Джарвиса (или метод заворачивания подарка) имеет временную сложность:  
- **В лучшем случае** (если выпуклая оболочка содержит **h = O(1)** вершин): **O(n)**.  
- **В худшем случае** (если все точки лежат на выпуклой оболочке, **h = n**): **O(n²)**.  

**Итоговая сложность:** **O(nh)**, где **h** — число вершин выпуклой оболочки.  

---

### 2) **Выбор худшего случая для алгоритма**  
Худший случай — когда **все точки уже лежат на выпуклой оболочке** (например, точки на окружности).  
- Тогда алгоритм проверяет **каждую точку для каждого шага**, что даёт **O(n²)**.  

**Пример:**  
- Точки: вершины правильного **n-угольника**.  
- На каждом шаге алгоритм просматривает **все оставшиеся точки**.  

---

### 3) **Доказательство сложности алгоритма**  
1. **Нахождение начальной точки** (самой левой нижней): **O(n)**.  
2. **Построение оболочки:**  
   - Для **каждой из h вершин оболочки** ищем следующую точку за **O(n)** операций.  
   - Итого: **O(nh)**.  

**Если h ≈ n (все точки на оболочке), то O(n²).**  

---

### 4) **Принцип работы алгоритма**  
Алгоритм Джарвиса строит выпуклую оболочку, **"заворачивая"** множество точек:  
1. **Находим стартовую точку** (обычно самую левую нижнюю).  
2. **Последовательно ищем следующую точку оболочки**, которая образует **минимальный полярный угол** с предыдущей.  
3. **Повторяем**, пока не вернёмся в начальную точку.  

**Ключевая идея:**  
- На каждом шаге выбирается точка, которая **"самая крайняя"** относительно текущего направления.  
- Используется **скалярное произведение** или **векторное сравнение углов**.  

---

### 5) **Пошаговое выполнение алгоритма**  
**Пример:** Точки A(0,0), B(1,1), C(2,2), D(3,1), E(1,-1).  

1. **Шаг 1:** Находим самую левую нижнюю точку — **E(1,-1)**.  
2. **Шаг 2:** Ищем следующую точку оболочки:  
   - Сравниваем углы **относительно E**:  
     - Угол **EA** (A(0,0)) — минимальный.  
   - Добавляем **A** в оболочку.  
3. **Шаг 3:** Теперь ищем точку из **A**:  
   - Угол **AB** (B(1,1)) — минимальный.  
   - Добавляем **B**.  
4. **Шаг 4:** Из **B** ищем следующую:  
   - Угол **BC** (C(2,2)) — минимальный.  
   - Добавляем **C**.  
5. **Шаг 5:** Из **C** следующая точка — **D(3,1)** (угол CD).  
6. **Шаг 6:** Из **D** возвращаемся в **E** — оболочка замкнута.  

**Результат:** Оболочка — **E → A → B → C → D → E**.  

---

### 6) **Результат работы алгоритма**  
- **Выпуклая оболочка** — минимальный выпуклый многоугольник, содержащий все точки.  
- Для примера:  
  - Точки оболочки: **E(1,-1), A(0,0), B(1,1), C(2,2), D(3,1)**.  
  - Форма — выпуклый пятиугольник.  

**Вывод:**  
- Алгоритм Джарвиса **прост в реализации**, но **неэффективен** при большом **h** (O(n²) в худшем случае).  
- Подходит для **малых наборов данных** или случаев, когда **h мало**.  
- В реальных задачах чаще используют **алгоритм Грэхема (O(n log n))** или **алгоритм Эндрю (O(n log n))**.

## **Алгоритм Чана**

### **1) Выбор временной сложности алгоритма**  
Алгоритм Чана — **оптимальный алгоритм** построения выпуклой оболочки, сочетающий идеи алгоритмов Джарвиса и Грэхема.  

- **Временная сложность:**  
  - **В худшем случае:** **O(n log h)**, где **n** — количество точек, **h** — количество вершин выпуклой оболочки.  
  - **На практике:** работает быстрее алгоритма Грэхема (O(n log n)) при **h ≪ n**.  

---

### **2) Выбор худшего случая для алгоритма**  
- Худший случай — когда **все точки лежат на выпуклой оболочке (h = n)**, тогда сложность стремится к **O(n log n)** (как у алгоритма Грэхема).  
- Однако в большинстве случаев **h ≪ n**, и алгоритм работает за **O(n log h)**.  

---

### **3) Доказательство сложности алгоритма**  
Алгоритм Чана состоит из двух фаз:  
1. **Разбиение на группы** (размером **m**) и построение локальных оболочек:  
   - Число групп: **n/m**.  
   - Построение локальных оболочек алгоритмом Грэхема: **O((n/m) ⋅ m log m) = O(n log m)**.  
2. **Объединение оболочек алгоритмом Джарвиса**:  
   - На каждом из **h** шагов обрабатывается **n/m** групп.  
   - Итоговая сложность: **O(h ⋅ (n/m) log m)**.  

**Оптимальный выбор m = h:**  
- Общая сложность: **O(n log h)**.  

---

### **4) Принцип работы алгоритма**  
Алгоритм Чана комбинирует:  
- **Разделение точек** на небольшие группы (размером **m**).  
- **Построение выпуклых оболочек** для каждой группы (алгоритмом Грэхема).  
- **Постепенное "заворачивание"** (как в алгоритме Джарвиса), но с использованием уже готовых локальных оболочек для ускорения.  

**Ключевые идеи:**  
1. **Локальные оболочки** уменьшают число точек, которые нужно проверять на каждом шаге.  
2. **Алгоритм Джарвиса** применяется не ко всем точкам, а только к вершинам локальных оболочек.  

---

### **5) Пошаговое выполнение алгоритма**  
**Пример:** Точки A(0,0), B(1,1), C(2,2), D(3,1), E(1,-1), F(4,0).  

#### **Шаг 1. Разбиение на группы (например, m = 3)**  
- Группа 1: A, B, C → Оболочка: A → B → C.  
- Группа 2: D, E, F → Оболочка: E → D → F.  

#### **Шаг 2. Построение глобальной оболочки (алгоритм Джарвиса)**  
1. Находим стартовую точку — **E(1,-1)**.  
2. Ищем следующую точку:  
   - Среди вершин **A, B, C** и **D, F** выбираем **A(0,0)** (минимальный угол).  
3. Следующая точка:  
   - Из **A** ищем точку с минимальным углом → **B(1,1)**.  
4. Далее:  
   - Из **B** → **C(2,2)**.  
   - Из **C** → **D(3,1)**.  
   - Из **D** → **F(4,0)**.  
   - Из **F** возвращаемся в **E**.  

**Итоговая оболочка:** **E → A → B → C → D → F → E**.  

---

### **6) Результат работы алгоритма**  
- **Выпуклая оболочка** — минимальный выпуклый многоугольник, содержащий все точки.  
- Для примера:  
  - Вершины: **E(1,-1), A(0,0), B(1,1), C(2,2), D(3,1), F(4,0)**.  
  - Форма — выпуклый шестиугольник.  

**Вывод:**  
- Алгоритм Чана **адаптивен** — чем меньше **h**, тем он эффективнее.  
- В **лучшем случае** (маленькая оболочка) работает почти за **линейное время**.  
- В **худшем случае** (h ≈ n) сводится к **O(n log n)** (как алгоритм Грэхема).  
- **Оптимален** для задач, где **h заранее неизвестно**, но предполагается, что **h ≪ n**.  

### **Сравнение с другими алгоритмами**  
| Алгоритм  | Сложность       | Применимость                     |
|-----------|----------------|----------------------------------|
| Джарвис   | O(nh) → O(n²)  | Медленный, но простой            |
| Грэхем    | O(n log n)     | Универсальный, стабильный        |
| Чан       | **O(n log h)** | Лучший, когда h мало или неизвестно |  

**Итог:**  
- Если **h мало** → Чан выигрывает.  
- Если **h ≈ n** → Грэхем или быстрая оболочка (Эндрю).  
- Если **очень мало точек** → можно использовать Джарвис.